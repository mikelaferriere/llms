
version: '3.8'
services:

  # https://github.com/oobabooga/text-generation-webui
  text-generation-webui:
    container_name: text-generation-webui
    restart: unless-stopped
    user: 1000:1000
    # This is to disable watchtower if it is being run on the current system
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
    build:
      context: text-generation-webui
      dockerfile: docker/nvidia/Dockerfile
      args:
        # specify which cuda version your card supports: https://developer.nvidia.com/cuda-gpus
        TORCH_CUDA_ARCH_LIST: ${TORCH_CUDA_ARCH_LIST:-7.5}
        # WEBUI_VERSION: ${WEBUI_VERSION:-HEAD}

        # Comma separated extensions to build
        # BUILD_EXTENSIONS: "openai"

        APP_GID: 1000
        APP_UID: 1000
    environment:
      # by default the Dockerfile specifies these versions: 3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX
      # however for me to work i had to specify the exact version for my card ( 2060 ) it was 7.5
      # https://developer.nvidia.com/cuda-gpus you can find the version for your card here
      TORCH_CUDA_ARCH_LIST: 7.5

      # the port the webui binds to on the host
      HOST_PORT: 7860

      # the port the webui binds to inside the container
      CONTAINER_PORT: 7860

      # the port the api binds to on the host
      # HOST_STREAMING_PORT: 5005

      # the port the api binds to inside the container
      # CONTAINER_STREAMING_PORT: 5005

      # Comma separated extensions to build
      BUILD_EXTENSIONS: "api,openai"

      # Set APP_RUNTIME_GID to an appropriate host system group to enable access to mounted volumes
      # You can find your current host user group id with the command `id -g`
      APP_RUNTIME_GID: 1000

      # override default app build permissions (handy for deploying to cloud)
      APP_GID: 1000
      APP_UID: 1000

      # You will first need to run this "bare" CLI_ARGs so you can download the models via the UI.
      # Once you do that, you can specify the model directly and it will load up on start.
      # You can also download the models outside of this container in the proper location (see volumes)

      # NOTE: CLI_ARGS are no longer respected. Update the text-generation-webui/CMD_FLAGS.txt with the
      #       clie args

      # CLI_ARGS: "--api --listen

      # Set cache env
      HF_HOME: /home/app/text-generation-webui/cache/

    ports:
      - "${HOST_PORT:-7860}:${CONTAINER_PORT:-7860}"
      - "${HOST_API_PORT:-5000}:${CONTAINER_API_PORT:-5000}"
      # - "${HOST_STREAMING_PORT:-5005}:${CONTAINER_STREAMING_PORT:-5005}"
    # stdin_open: true
    # tty: true

    volumes:
      - ./text-generation-webui/cache:/home/app/text-generation-webui/cache
      - ./text-generation-webui/characters:/home/app/text-generation-webui/characters
      - ./text-generation-webui/extensions:/home/app/text-generation-webui/extensions
      - ./text-generation-webui/loras:/home/app/text-generation-webui/loras
      - ./text-generation-webui/logs:/home/app/text-generation-webui/logs
      - ./text-generation-webui/models:/home/app/text-generation-webui/models
      - ./text-generation-webui/presets:/home/app/text-generation-webui/presets
      - ./text-generation-webui/prompts:/home/app/text-generation-webui/prompts
      - ./text-generation-webui/softprompts:/home/app/text-generation-webui/softprompts
      - ./text-generation-webui/training:/home/app/text-generation-webui/training
      - ./text-generation-webui/cloudflared:/etc/cloudflared
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2']
              capabilities: [gpu]
